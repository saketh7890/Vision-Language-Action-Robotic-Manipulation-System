# Vision-Language-Action-Robotic-Manipulation-System
ğŸš€ Project Overview â€¢ Excited to share my final project for RAS 545: Robotics & AI Systems at Arizona State University, completed under the guidance of Professor Sangram Redkar.  â€¢ I developed an LLM-powered robotic manipulation system that enables a Dobot robotic arm to interpret natural-language commands and autonomously execute multi-step pick-and-place tasks. ğŸ§  Intelligent Language-to-Action System â€¢ Integrated a Groq-hosted Llama 3.3 model to understand prompts such as â€œpick the farthest red blockâ€ or â€œplace the blue block next to the tallest stack.â€  â€¢ The LLM generates safe, validated Python actions based on a strict manipulation rule set and an AST-secured execution environment.  â€¢ The system supports contextual reasoning, allowing references like â€œthat blockâ€ or â€œplace it where it was before.â€ ğŸ¯ Vision & Calibration â€¢ Designed a complete OpenCV color-detection pipeline to identify and track blocks on the workspace.  â€¢ Applied an affine calibration model to map pixel coordinates to precise Dobot coordinates for reliable manipulation. ğŸ“¦ Agentic Memory & World Modeling â€¢ Built persistent memory to track:  â€“ block positions and identities  â€“ stack heights and configurations  â€“ the current gripper state  â€“ last manipulated block  â€“ command history  â€¢ Enables multi-step reasoning and tasks involving stacks, adjacency, and spatial relationships. ğŸ¤– Robotic Execution â€¢ Implemented a safe, height-aware pick-and-place controller with suction gripping and smooth motion.  â€¢ Ensured deterministic, safe execution through code validation and controlled function whitelisting.
